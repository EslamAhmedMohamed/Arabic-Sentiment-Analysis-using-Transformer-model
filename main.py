!pip install tashaphyne
import gensim
from gensim.models import KeyedVectors
from gensim.models import word2vec

!unzip '/kaggle/working/AraVec'
!rm AraVec
import nltk
import pandas as pd
import torch
from sklearn.feature_extraction.text import TfidfVectorizer
from tashaphyne.stemming import ArabicLightStemmer
import re
import emoji
from nltk.corpus import stopwords
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import LSTM, Dense, Embedding
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import TensorDataset
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f'There are {torch.cuda.device_count()} GPU(s) available.')
    print('Device name:', torch.cuda.get_device_name(0))

else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")
from sklearn.preprocessing import LabelEncoder
nltk.download('stopwords')
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Data preprocessing ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

test=pd.read_csv("/kaggle/input/neural/test_no_label.csv")
df=pd.read_csv("/kaggle/input/neural/train1.csv")
#print(df)
df = df.iloc[:]
# print("print data", df)
# print(len(df))
# Remove duplicated
df.review_description.duplicated().sum()
df.drop(df[df.review_description.duplicated() == True].index, axis=0, inplace=True)


# Remove Punctuation
df.review_description = df.review_description.astype(str)
df.review_description = df.review_description.apply(
    lambda x: re.sub('[%s]' % re.escape("""!"#$%&'()*+,ØŒ-./:;<=>ØŸ?@[\]^_`{|}~"""), ' ', x))
df.review_description = df.review_description.apply(lambda x: x.replace('Ø›', "", ))
# print("print data after pun", df.head())


# Define the function to remove consecutive duplicated Arabic words
def remove_duplicate_arabic_words(text):
    # Tokenize the text into words
    words = text.split()

    # Remove consecutive duplicated words
    unique_words = [words[i] for i in range(len(words)) if i == 0 or words[i] != words[i - 1]]

    # Join the unique words back into a sentence
    modified_text = ' '.join(unique_words)

    return modified_text


df['review_description'] = df['review_description'].apply(remove_duplicate_arabic_words)
# Remove StopWords
stopWords = list(set(stopwords.words("arabic")))  ## To remove duplictes and return to list again

# Some words needed to work with to will remove
for word in ['Ù„Ø§', 'Ù„ÙƒÙ†', 'ÙˆÙ„ÙƒÙ†']:
    stopWords.remove(word)
df.review_description = df.review_description.apply(
    lambda x: " ".join([word for word in x.split() if word not in stopWords]))
# print("print data after removing stop words", df.head())

# Replace Emoji by Text
emojis = {"ğŸ™‚": "ÙŠØ¨ØªØ³Ù…", "ğŸ˜‚": "ÙŠØ¶Ø­Ùƒ", "ğŸ’”": "Ù‚Ù„Ø¨ Ø­Ø²ÙŠÙ†", "ğŸ™‚": "ÙŠØ¨ØªØ³Ù…", "â¤": "Ø­Ø¨", "â¤": "Ø­Ø¨", "ğŸ˜": "Ø­Ø¨", "ğŸ˜­": "ÙŠØ¨ÙƒÙŠ",
          "ğŸ˜¢": "Ø­Ø²Ù†", "ğŸ˜”": "Ø­Ø²Ù†", "â™¥": "Ø­Ø¨", "ğŸ’œ": "Ø­Ø¨", "ğŸ˜…": "ÙŠØ¶Ø­Ùƒ", "ğŸ™": "Ø­Ø²ÙŠÙ†", "ğŸ’•": "Ø­Ø¨", "ğŸ’™": "Ø­Ø¨", "ğŸ˜": "Ø­Ø²ÙŠÙ†",
          "ğŸ˜Š": "Ø³Ø¹Ø§Ø¯Ø©", "ğŸ‘": "ÙŠØµÙÙ‚", "ğŸ‘Œ": "Ø§Ø­Ø³Ù†Øª", "ğŸ˜´": "ÙŠÙ†Ø§Ù…", "ğŸ˜€": "ÙŠØ¶Ø­Ùƒ", "ğŸ˜Œ": "Ø­Ø²ÙŠÙ†", "ğŸŒ¹": "ÙˆØ±Ø¯Ø©", "ğŸ™ˆ": "Ø­Ø¨",
          "ğŸ˜„": "ÙŠØ¶Ø­Ùƒ", "ğŸ˜": "Ù…Ø­Ø§ÙŠØ¯", "âœŒ": "Ù…Ù†ØªØµØ±", "âœ¨": "Ù†Ø¬Ù…Ù‡", "ğŸ¤”": "ØªÙÙƒÙŠØ±", "ğŸ˜": "ÙŠØ³ØªÙ‡Ø²Ø¡", "ğŸ˜’": "ÙŠØ³ØªÙ‡Ø²Ø¡", "ğŸ™„": "Ù…Ù„Ù„",
          "ğŸ˜•": "Ø¹ØµØ¨ÙŠØ©", "ğŸ˜ƒ": "ÙŠØ¶Ø­Ùƒ", "ğŸŒ¸": "ÙˆØ±Ø¯Ø©", "ğŸ˜“": "Ø­Ø²Ù†", "ğŸ’": "Ø­Ø¨", "ğŸ’—": "Ø­Ø¨", "ğŸ˜‘": "Ù…Ù†Ø²Ø¹Ø¬", "ğŸ’­": "ØªÙÙƒÙŠØ±",
          "ğŸ˜": "Ø«Ù‚Ø©", "ğŸ’›": "Ø­Ø¨", "ğŸ˜©": "Ø­Ø²ÙŠÙ†", "ğŸ’ª": "Ø¹Ø¶Ù„Ø§Øª", "ğŸ‘": "Ù…ÙˆØ§ÙÙ‚", "ğŸ™ğŸ»": "Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨", "ğŸ˜³": "Ù…ØµØ¯ÙˆÙ…", "ğŸ‘ğŸ¼": "ØªØµÙÙŠÙ‚",
          "ğŸ¶": "Ù…ÙˆØ³ÙŠÙ‚ÙŠ", "ğŸŒš": "ØµÙ…Øª", "ğŸ’š": "Ø­Ø¨", "ğŸ™": "Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨", "ğŸ’˜": "Ø­Ø¨", "ğŸƒ": "Ø³Ù„Ø§Ù…", "â˜º": "ÙŠØ¶Ø­Ùƒ", "ğŸ¸": "Ø¶ÙØ¯Ø¹",
          "ğŸ˜¶": "Ù…ØµØ¯ÙˆÙ…", "âœŒ": "Ù…Ø±Ø­", "âœ‹ğŸ»": "ØªÙˆÙ‚Ù", "ğŸ˜‰": "ØºÙ…Ø²Ø©", "ğŸŒ·": "Ø­Ø¨", "ğŸ™ƒ": "Ù…Ø¨ØªØ³Ù…", "ğŸ˜«": "Ø­Ø²ÙŠÙ†", "ğŸ˜¨": "Ù…ØµØ¯ÙˆÙ…",
          "ğŸ¼ ": "Ù…ÙˆØ³ÙŠÙ‚ÙŠ", "ğŸ": "Ù…Ø±Ø­", "ğŸ‚": "Ù…Ø±Ø­", "ğŸ’Ÿ": "Ø­Ø¨", "ğŸ˜ª": "Ø­Ø²Ù†", "ğŸ˜†": "ÙŠØ¶Ø­Ùƒ", "ğŸ˜£": "Ø§Ø³ØªÙŠØ§Ø¡", "â˜º": "Ø­Ø¨",
          "ğŸ˜±": "ÙƒØ§Ø±Ø«Ø©", "ğŸ˜": "ÙŠØ¶Ø­Ùƒ", "ğŸ˜–": "Ø§Ø³ØªÙŠØ§Ø¡", "ğŸƒğŸ¼": "ÙŠØ¬Ø±ÙŠ", "ğŸ˜¡": "ØºØ¶Ø¨", "ğŸš¶": "ÙŠØ³ÙŠØ±", "ğŸ¤•": "Ù…Ø±Ø¶", "â€¼": "ØªØ¹Ø¬Ø¨",
          "ğŸ•Š": "Ø·Ø§Ø¦Ø±", "ğŸ‘ŒğŸ»": "Ø§Ø­Ø³Ù†Øª", "â£": "Ø­Ø¨", "ğŸ™Š": "Ù…ØµØ¯ÙˆÙ…", "ğŸ’ƒ": "Ø³Ø¹Ø§Ø¯Ø© Ù…Ø±Ø­", "ğŸ’ƒğŸ¼": "Ø³Ø¹Ø§Ø¯Ø© Ù…Ø±Ø­", "ğŸ˜œ": "Ù…Ø±Ø­",
          "ğŸ‘Š": "Ø¶Ø±Ø¨Ø©", "ğŸ˜Ÿ": "Ø§Ø³ØªÙŠØ§Ø¡", "ğŸ’–": "Ø­Ø¨", "ğŸ˜¥": "Ø­Ø²Ù†", "ğŸ»": "Ù…ÙˆØ³ÙŠÙ‚ÙŠ", "âœ’": "ÙŠÙƒØªØ¨", "ğŸš¶ğŸ»": "ÙŠØ³ÙŠØ±", "ğŸ’": "Ø§Ù„Ù…Ø§Ø¸",
          "ğŸ˜·": "ÙˆØ¨Ø§Ø¡ Ù…Ø±Ø¶", "â˜": "ÙˆØ§Ø­Ø¯", "ğŸš¬": "ØªØ¯Ø®ÙŠÙ†", "ğŸ’": "ÙˆØ±Ø¯", "ğŸŒ": "Ø´Ù…Ø³", "ğŸ‘†": "Ø§Ù„Ø§ÙˆÙ„", "âš ": "ØªØ­Ø°ÙŠØ±",
          "ğŸ¤—": "Ø§Ø­ØªÙˆØ§Ø¡", "âœ–": "ØºÙ„Ø·", "ğŸ“": "Ù…ÙƒØ§Ù†", "ğŸ‘¸": "Ù…Ù„ÙƒÙ‡", "ğŸ‘‘": "ØªØ§Ø¬", "âœ”": "ØµØ­", "ğŸ’Œ": "Ù‚Ù„Ø¨", "ğŸ˜²": "Ù…Ù†Ø¯Ù‡Ø´",
          "ğŸ’¦": "Ù…Ø§Ø¡", "ğŸš«": "Ø®Ø·Ø§", "ğŸ‘ğŸ»": "Ø¨Ø±Ø§ÙÙˆ", "ğŸŠ": "ÙŠØ³Ø¨Ø­", "ğŸ‘ğŸ»": "ØªÙ…Ø§Ù…", "â­•": "Ø¯Ø§Ø¦Ø±Ù‡ ÙƒØ¨ÙŠØ±Ù‡", "ğŸ·": "Ø³Ø§ÙƒØ³ÙÙˆÙ†",
          "ğŸ‘‹": "ØªÙ„ÙˆÙŠØ­ Ø¨Ø§Ù„ÙŠØ¯", "âœŒğŸ¼": "Ø¹Ù„Ø§Ù…Ù‡ Ø§Ù„Ù†ØµØ±", "ğŸŒ": "Ù…Ø¨ØªØ³Ù…", "â¿": "Ø¹Ù‚Ø¯Ù‡ Ù…Ø²Ø¯ÙˆØ¬Ù‡", "ğŸ’ªğŸ¼": "Ù‚ÙˆÙŠ", "ğŸ“©": "ØªÙˆØ§ØµÙ„ Ù…Ø¹ÙŠ",
          "â˜•": "Ù‚Ù‡ÙˆÙ‡", "ğŸ˜§": "Ù‚Ù„Ù‚ Ùˆ ØµØ¯Ù…Ø©", "ğŸ—¨": "Ø±Ø³Ø§Ù„Ø©", "â—": "ØªØ¹Ø¬Ø¨", "ğŸ™†ğŸ»": "Ø§Ø´Ø§Ø±Ù‡ Ù…ÙˆØ§ÙÙ‚Ù‡", "ğŸ‘¯": "Ø§Ø®ÙˆØ§Øª", "Â©": "Ø±Ù…Ø²",
          "ğŸ‘µğŸ½": "Ø³ÙŠØ¯Ù‡ Ø¹Ø¬ÙˆØ²Ù‡", "ğŸ£": "ÙƒØªÙƒÙˆØª", "ğŸ™Œ": "ØªØ´Ø¬ÙŠØ¹", "ğŸ™‡": "Ø´Ø®Øµ ÙŠÙ†Ø­Ù†ÙŠ", "ğŸ‘ğŸ½": "Ø§ÙŠØ¯ÙŠ Ù…ÙØªÙˆØ­Ù‡", "ğŸ‘ŒğŸ½": "Ø¨Ø§Ù„Ø¸Ø¨Ø·",
          "â‰": "Ø§Ø³ØªÙ†ÙƒØ§Ø±", "âš½": "ÙƒÙˆØ±Ù‡", "ğŸ•¶": "Ø­Ø¨", "ğŸˆ": "Ø¨Ø§Ù„ÙˆÙ†", "ğŸ€": "ÙˆØ±Ø¯Ù‡", "ğŸ’µ": "ÙÙ„ÙˆØ³", "ğŸ˜‹": "Ø¬Ø§Ø¦Ø¹", "ğŸ˜›": "ÙŠØºÙŠØ¸",
          "ğŸ˜ ": "ØºØ§Ø¶Ø¨", "âœğŸ»": "ÙŠÙƒØªØ¨", "ğŸŒ¾": "Ø§Ø±Ø²", "ğŸ‘£": "Ø§Ø«Ø± Ù‚Ø¯Ù…ÙŠÙ†", "âŒ": "Ø±ÙØ¶", "ğŸŸ": "Ø·Ø¹Ø§Ù…", "ğŸ‘¬": "ØµØ¯Ø§Ù‚Ø©", "ğŸ°": "Ø§Ø±Ù†Ø¨",
          "â˜‚": "Ù…Ø·Ø±", "âšœ": "Ù…Ù…Ù„ÙƒØ© ÙØ±Ù†Ø³Ø§", "ğŸ‘": "Ø®Ø±ÙˆÙ", "ğŸ—£": "ØµÙˆØª Ù…Ø±ØªÙØ¹", "ğŸ‘ŒğŸ¼": "Ø§Ø­Ø³Ù†Øª", "â˜˜": "Ù…Ø±Ø­", "ğŸ˜®": "ØµØ¯Ù…Ø©",
          "ğŸ˜¦": "Ù‚Ù„Ù‚", "â­•": "Ø§Ù„Ø­Ù‚", "âœ": "Ù‚Ù„Ù…", "â„¹": "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª", "ğŸ™ğŸ»": "Ø±ÙØ¶", "âšª": "Ù†Ø¶Ø§Ø±Ø© Ù†Ù‚Ø§Ø¡", "ğŸ¤": "Ø­Ø²Ù†", "ğŸ’«": "Ù…Ø±Ø­",
          "ğŸ’": "Ø­Ø¨", "ğŸ”": "Ø·Ø¹Ø§Ù…", "â¤": "Ø­Ø¨", "âœˆ": "Ø³ÙØ±", "ğŸƒğŸ»â€â™€": "ÙŠØ³ÙŠØ±", "ğŸ³": "Ø°ÙƒØ±", "ğŸ¤": "Ù…Ø§ÙŠÙƒ ØºÙ†Ø§Ø¡", "ğŸ¾": "ÙƒØ±Ù‡",
          "ğŸ”": "Ø¯Ø¬Ø§Ø¬Ø©", "ğŸ™‹": "Ø³Ø¤Ø§Ù„", "ğŸ“®": "Ø¨Ø­Ø±", "ğŸ’‰": "Ø¯ÙˆØ§Ø¡", "ğŸ™ğŸ¼": "Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨", "ğŸ’‚ğŸ¿ ": "Ø­Ø§Ø±Ø³", "ğŸ¬": "Ø³ÙŠÙ†Ù…Ø§",
          "â™¦": "Ù…Ø±Ø­", "ğŸ’¡": "Ù‚ÙƒØ±Ø©", "â€¼": "ØªØ¹Ø¬Ø¨", "ğŸ‘¼": "Ø·ÙÙ„", "ğŸ”‘": "Ù…ÙØªØ§Ø­", "â™¥": "Ø­Ø¨", "ğŸ•‹": "ÙƒØ¹Ø¨Ø©", "ğŸ“": "Ø¯Ø¬Ø§Ø¬Ø©",
          "ğŸ’©": "Ù…Ø¹ØªØ±Ø¶", "ğŸ‘½": "ÙØ¶Ø§Ø¦ÙŠ", "â˜”": "Ù…Ø·Ø±", "ğŸ·": "Ø¹ØµÙŠØ±", "ğŸŒŸ": "Ù†Ø¬Ù…Ø©", "â˜": "Ø³Ø­Ø¨", "ğŸ‘ƒ": "Ù…Ø¹ØªØ±Ø¶", "ğŸŒº": "Ù…Ø±Ø­",
          "ğŸ”ª": "Ø³ÙƒÙŠÙ†Ø©", "â™¨": "Ø³Ø®ÙˆÙ†ÙŠØ©", "ğŸ‘ŠğŸ¼": "Ø¶Ø±Ø¨", "âœ": "Ù‚Ù„Ù…", "ğŸš¶ğŸ¾â€â™€": "ÙŠØ³ÙŠØ±", "ğŸ‘Š": "Ø¶Ø±Ø¨Ø©", "â—¾": "ÙˆÙ‚Ù", "ğŸ˜š": "Ø­Ø¨",
          "ğŸ”¸": "Ù…Ø±Ø­", "ğŸ‘ğŸ»": "Ù„Ø§ ÙŠØ¹Ø¬Ø¨Ù†ÙŠ", "ğŸ‘ŠğŸ½": "Ø¶Ø±Ø¨Ø©", "ğŸ˜™": "Ø­Ø¨", "ğŸ¥": "ØªØµÙˆÙŠØ±", "ğŸ‘‰": "Ø¬Ø°Ø¨ Ø§Ù†ØªØ¨Ø§Ù‡", "ğŸ‘ğŸ½": "ÙŠØµÙÙ‚",
          "ğŸ’ªğŸ»": "Ø¹Ø¶Ù„Ø§Øª", "ğŸ´": "Ø§Ø³ÙˆØ¯", "ğŸ”¥": "Ø­Ø±ÙŠÙ‚", "ğŸ˜¬": "Ø¹Ø¯Ù… Ø§Ù„Ø±Ø§Ø­Ø©", "ğŸ‘ŠğŸ¿": "ÙŠØ¶Ø±Ø¨", "ğŸŒ¿": "ÙˆØ±Ù‚Ù‡ Ø´Ø¬Ø±Ù‡", "âœ‹ğŸ¼": "ÙƒÙ Ø§ÙŠØ¯",
          "ğŸ‘": "Ø§ÙŠØ¯ÙŠ Ù…ÙØªÙˆØ­Ù‡", "â˜ ": "ÙˆØ¬Ù‡ Ù…Ø±Ø¹Ø¨", "ğŸ‰": "ÙŠÙ‡Ù†Ø¦", "ğŸ”•": "ØµØ§Ù…Øª", "ğŸ˜¿": "ÙˆØ¬Ù‡ Ø­Ø²ÙŠÙ†", "â˜¹": "ÙˆØ¬Ù‡ ÙŠØ§Ø¦Ø³", "ğŸ˜˜": "Ø­Ø¨",
          "ğŸ˜°": "Ø®ÙˆÙ Ùˆ Ø­Ø²Ù†", "ğŸŒ¼": "ÙˆØ±Ø¯Ù‡", "ğŸ’‹": "Ø¨ÙˆØ³Ù‡", "ğŸ‘‡": "Ù„Ø§Ø³ÙÙ„", "â£": "Ø­Ø¨", "ğŸ§": "Ø³Ù…Ø§Ø¹Ø§Øª", "ğŸ“": "ÙŠÙƒØªØ¨", "ğŸ˜‡": "Ø¯Ø§ÙŠØ®",
          "ğŸ˜ˆ": "Ø±Ø¹Ø¨", "ğŸƒ": "ÙŠØ¬Ø±ÙŠ", "âœŒğŸ»": "Ø¹Ù„Ø§Ù…Ù‡ Ø§Ù„Ù†ØµØ±", "ğŸ”«": "ÙŠØ¶Ø±Ø¨", "â—": "ØªØ¹Ø¬Ø¨", "ğŸ‘": "ØºÙŠØ± Ù…ÙˆØ§ÙÙ‚", "ğŸ”": "Ù‚ÙÙ„",
          "ğŸ‘ˆ": "Ù„Ù„ÙŠÙ…ÙŠÙ†", "â„¢": "Ø±Ù…Ø²", "ğŸš¶ğŸ½": "ÙŠØªÙ…Ø´ÙŠ", "ğŸ˜¯": "Ù…ØªÙØ§Ø¬Ø£", "âœŠ": "ÙŠØ¯ Ù…ØºÙ„Ù‚Ù‡", "ğŸ˜»": "Ø§Ø¹Ø¬Ø§Ø¨", "ğŸ™‰": "Ù‚Ø±Ø¯",
          "ğŸ‘§": "Ø·ÙÙ„Ù‡ ØµØºÙŠØ±Ù‡", "ğŸ”´": "Ø¯Ø§Ø¦Ø±Ù‡ Ø­Ù…Ø±Ø§Ø¡", "ğŸ’ªğŸ½": "Ù‚ÙˆÙ‡", "ğŸ’¤": "ÙŠÙ†Ø§Ù…", "ğŸ‘€": "ÙŠÙ†Ø¸Ø±", "âœğŸ»": "ÙŠÙƒØªØ¨", "â„": "ØªÙ„Ø¬",
          "ğŸ’€": "Ø±Ø¹Ø¨", "ğŸ˜¤": "ÙˆØ¬Ù‡ Ø¹Ø§Ø¨Ø³", "ğŸ–‹": "Ù‚Ù„Ù…", "ğŸ©": "ÙƒØ§Ø¨", "â˜•": "Ù‚Ù‡ÙˆÙ‡", "ğŸ˜¹": "Ø¶Ø­Ùƒ", "ğŸ’“": "Ø­Ø¨", "â˜„ ": "Ù†Ø§Ø±",
          "ğŸ‘»": "Ø±Ø¹Ø¨", "â": "Ø®Ø·Ø¡", "ğŸ¤®": "Ø­Ø²Ù†", 'ğŸ»': "Ø§Ø­Ù…Ø±"}
emoticons_to_emoji = {":)": "ğŸ™‚", ":(": "ğŸ™", "xD": "ğŸ˜†", ":=(": "ğŸ˜­", ":'(": "ğŸ˜¢", ":'â€‘(": "ğŸ˜¢", "XD": "ğŸ˜‚", ":D": "ğŸ™‚",
                      "â™¬": "Ù…ÙˆØ³ÙŠÙ‚ÙŠ", "â™¡": "â¤", "â˜»": "ğŸ™‚"}


def checkemojie(text):
    emojistext = []
    for char in text:
        if any(emoji.distinct_emoji_list(char)) and char in emojis.keys():
            emojistext.append(emojis[emoji.distinct_emoji_list(char)[0]])
    return " ".join(emojistext)


def emojiTextTransform(text):
    cleantext = re.sub(r'[^\w\s]', '', text)
    return cleantext + " " + checkemojie(text)


# Apply checkemojie and emojiTextTransform
df['review_description'] = df['review_description'].apply(lambda x: emojiTextTransform(x))
# print("print data after changing the emoji to text", df['review_description'].head())

# Remove Numbers
df.review_description = df.review_description.apply(lambda x: ''.join([word for word in x if not word.isdigit()]))

# Apply Stemming
arabic_stemmer = ArabicLightStemmer()
# Apply stemming to the 'review_description' column
df['review_description'] = df['review_description'].apply(
    lambda x: " ".join([arabic_stemmer.light_stem(word) for word in x.split()]))


hidden_dim =264 
output_size = 3 
num_layers = 2  
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initializations and corrections to your code
df.dropna(subset=['review_description'], inplace=True)
review_description = df['review_description']
y = df['rating']  # 1, 0, -1

tokenizer = Tokenizer()
tokenizer.fit_on_texts(review_description)
seq = tokenizer.texts_to_sequences(review_description)
seq_pad = pad_sequences(seq, maxlen=150, padding="post", truncating="post")
vocab_size = len(tokenizer.word_index) + 1

w2v_embeddings_index = {}
TOTAL_EMBEDDING_DIM = 300
embeddings_file = '/kaggle/working/full_grams_cbow_300_twitter.mdl'
w2v_model = KeyedVectors.load(embeddings_file)

for word in w2v_model.wv.index_to_key:
    w2v_embeddings_index[word] = w2v_model.wv[word]

embedding_matrix = np.zeros((vocab_size, TOTAL_EMBEDDING_DIM))
for word, i in tokenizer.word_index.items():
    embedding_vector = w2v_embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Transformer model ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Hyperparameters
max_len = 150
word_index = tokenizer.word_index
vocab_size = len(word_index) + 1
d_model = 150  
num_heads = 5 
num_layers = 2  
dropout_rate = 0.2


# Model definition
class PositionalEncoding(nn.Module):
    def _init_(self, d_model, max_len=150):
        super()._init_()
        pe = torch.zeros(max_len, d_model)

        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)


        self.register_buffer('pe', pe)

    def forward(self, x):
        x_pe = x + self.pe[:, :x.size(1)]
        return x_pe
    

class transformr_network(nn.Module):
    def _init_(self, vocab_size, d_model, nhead, dim_feedforward, num_layers, dropout, max_len=150):
        super()._init_()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, max_len)
        self.enc_layers = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)
        self.transformer_encoder = TransformerEncoder(self.enc_layers, num_layers)
        self.fc = nn.Linear(d_model, 3)
        self.dropout = nn.Dropout(dropout)
        self.d_model = d_model

    def forward(self, src):
        src = self.embedding(src) * math.sqrt(self.d_model)
        src = self.pos_encoder(src)
        output = self.transformer_encoder(src)
        output = output.mean(dim=1)
        output = self.dropout(output)
        output = self.fc(output)
        return output

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = transformr_network(vocab_size, d_model, num_heads, d_model*4 , num_layers, dropout_rate).to(device)

# Training code
lossfun = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=0.001)

from torch.utils.data import random_split

train_size = int(0.8 * len(dataset))
validation_size = len(dataset) - train_size
train_dataset, validation_dataset = random_split(dataset, [train_size, validation_size])

train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)
validation_dataloader = DataLoader(validation_dataset, batch_size=64, shuffle=False)
for epoch in range(1, 21):
    # Training phase
    model.train() 
    train_loss = 0
    correct_predictions = 0
    total_predictions = 0
    
    for inputs, targets in train_dataloader:
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = lossfun(outputs, targets)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        correct_predictions += (predicted == targets).sum().item()
        total_predictions += targets.size(0)
   
    train_accuracy = 100 * correct_predictions / total_predictions
    print(f"Epoch {epoch}, Training Loss: {train_loss / len(train_dataloader):.4f}, Training Accuracy: {train_accuracy:.2f}%")
    
    # Validation phase
    model.eval() 
    validation_loss = 0
    correct_predictions = 0
    total_predictions = 0
    
    with torch.no_grad():
        for inputs, targets in validation_dataloader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = lossfun(outputs, targets)
            validation_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            correct_predictions += (predicted == targets).sum().item()
            total_predictions += targets.size(0)
    
    validation_accuracy = 100 * correct_predictions / total_predictions
    print(f"Epoch {epoch}, Validation Loss: {validation_loss / len(validation_dataloader):.4f}, Validation Accuracy: {validation_accuracy:.2f}%")

test=pd.read_csv("/kaggle/input/neural/test _no_label.csv")
test['review_description'] = test['review_description'].apply(remove_duplicate_arabic_words)
test['review_description'] = test['review_description'].apply(
    lambda x: " ".join([word for word in x.split() if word not in stopWords]))
test['review_description'] = test['review_description'].apply(lambda x: emojiTextTransform(x))
test['review_description'] = test['review_description'].apply(lambda x: ''.join([word for word in x if not word.isdigit()]))
test['review_description'] = test['review_description'].apply(
    lambda x: " ".join([arabic_stemmer.light_stem(word) for word in x.split()]))

test_seq = tokenizer.texts_to_sequences(test['review_description'])
test_seq_pad = pad_sequences(test_seq, maxlen=150, padding="post", truncating="post")
X_test = torch.from_numpy(test_seq_pad).long()
X_test = X_test.to(device)

model.eval()
with torch.no_grad():
    output = model(X_test)
    predicted_classes = torch.argmax(output, dim=1)

predicted_labels = le.inverse_transform(predicted_classes.cpu().numpy())
test['predicted_label'] = predicted_labels


test.to_csv('predicted_labels_transform1.csv', columns=['ID', 'review_description', 'predicted_label'], index=False, encoding='utf-8-sig')